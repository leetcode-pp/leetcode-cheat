{
  "question": "609.find-duplicate-file-in-system",
  "companys": [
    "TODO"
  ],
  "tags": [
    "TODO"
  ],
  "reslove": [
    {
      "lang": "js",
      "code": "\n\n\n/*\n * @lc app=leetcode id=609 lang=javascript\n *\n * [609] Find Duplicate File in System\n *\n * https://leetcode.com/problems/find-duplicate-file-in-system/description/\n *\n * algorithms\n * Medium (54.21%)\n * Total Accepted:    24.1K\n * Total Submissions: 44.2K\n * Testcase Example:  '[\"root/a 1.txt(abcd) 2.txt(efgh)\",\"root/c 3.txt(abcd)\",\"root/c/d 4.txt(efgh)\",\"root 4.txt(efgh)\"]'\n *\n * Given a list of directory info including directory path, and all the files\n * with contents in this directory, you need to find out all the groups of\n * duplicate files in the file system in terms of their paths.\n *\n * A group of duplicate files consists of at least two files that have exactly\n * the same content.\n *\n * A single directory info string in the input list has the following format:\n *\n * \"root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ...\n * fn.txt(fn_content)\"\n *\n * It means there are n files (f1.txt, f2.txt ... fn.txt with content\n * f1_content, f2_content ... fn_content, respectively) in directory\n * root/d1/d2/.../dm. Note that n >= 1 and m >= 0. If m = 0, it means the\n * directory is just the root directory.\n *\n * The output is a list of group of duplicate file paths. For each group, it\n * contains all the file paths of the files that have the same content. A file\n * path is a string that has the following format:\n *\n * \"directory_path/file_name.txt\"\n *\n * Example 1:\n *\n *\n * Input:\n * [\"root/a 1.txt(abcd) 2.txt(efgh)\", \"root/c 3.txt(abcd)\", \"root/c/d\n * 4.txt(efgh)\", \"root 4.txt(efgh)\"]\n * Output:\n *\n * [[\"root/a/2.txt\",\"root/c/d/4.txt\",\"root/4.txt\"],[\"root/a/1.txt\",\"root/c/3.txt\"]]\n *\n *\n *\n *\n * Note:\n *\n *\n * No order is required for the final output.\n * You may assume the directory name, file name and file content only has\n * letters and digits, and the length of file content is in the range of\n * [1,50].\n * The number of files given is in the range of [1,20000].\n * You may assume no files or directories share the same name in the same\n * directory.\n * You may assume each given directory info represents a unique directory.\n * Directory path and file info are separated by a single blank space.\n *\n *\n *\n * Follow-up beyond contest:\n *\n *\n * Imagine you are given a real file system, how will you search files? DFS or\n * BFS?\n * If the file content is very large (GB level), how will you modify your\n * solution?\n * If you can only read the file by 1kb each time, how will you modify your\n * solution?\n * What is the time complexity of your modified solution? What is the most\n * time-consuming part and memory consuming part of it? How to optimize?\n * How to make sure the duplicated files you find are not false positive?\n *\n *\n */\n/**\n * @param {string[]} paths\n * @return {string[][]}\n */\nvar findDuplicate = function(paths) {\n  const hashmap = {};\n\n  for (let path of paths) {\n    const [folder, ...files] = path.split(\" \");\n    for (let file of files) {\n      const lpi = file.indexOf(\"(\");\n      const rpi = file.lastIndexOf(\")\");\n      const filename = file.slice(0, lpi);\n      const content = file.slice(lpi, rpi);\n      const fullname = `${folder}/${filename}`;\n      if (!hashmap[content]) hashmap[content] = [];\n      hashmap[content].push(fullname);\n    }\n  }\n\n  return Object.values(hashmap).filter(q => q.length >= 2);\n};\n"
    }
  ]
}